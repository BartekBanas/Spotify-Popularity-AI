{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for estimating song's popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Utwórz nową kolumnę \"genre_id\" i przypisz wartości numeryczne do każdego unikalnego gatunku\n",
    "data['artists_id'], _ = pd.factorize(data['artists'])\n",
    "\n",
    "# Zapisz zmodyfikowane dane do pliku CSV\n",
    "data.to_csv('dataset.csv', index=False)\n",
    "\n",
    "data = data.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Nacho Vegas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 13\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Podział danych na zbiór treningowy i testowy\u001B[39;00m\n\u001B[0;32m     10\u001B[0m train_data, test_data, train_labels, test_labels \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[0;32m     11\u001B[0m     data[columns \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(one_hot\u001B[38;5;241m.\u001B[39mcolumns)], data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpopularity\u001B[39m\u001B[38;5;124m'\u001B[39m], test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m test_data \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(test_data, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     15\u001B[0m train_labels \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(train_labels, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Nacho Vegas'"
     ]
    }
   ],
   "source": [
    "columns = ['artists','track_name','popularity','duration_ms','explicit','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature','genre_id']\n",
    "\n",
    "# Kodowanie one-hot dla kolumny 'track_genre'\n",
    "one_hot = pd.get_dummies(data['track_genre'], prefix='genre')\n",
    "data = pd.concat([data, one_hot], axis=1)\n",
    "\n",
    "amountOfFeatures = len(columns) + len(one_hot.columns)\n",
    "\n",
    "# Podział danych na zbiór treningowy i testowy\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data[columns + list(one_hot.columns)], data['popularity'], test_size=0.2)\n",
    "\n",
    "train_data = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "test_data = tf.convert_to_tensor(test_data, dtype=tf.float32)\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "mean = train_data.numpy().mean(axis=0)\n",
    "std = train_data.numpy().std(axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(amountOfFeatures + 1, activation='relu', input_shape=[len(columns)]),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures, activation='relu'),\n",
    "    tf.keras.layers.Dense(amountOfFeatures / 2, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              metrics=['mae', 'mse'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trenowanie modelu\n",
    "history = model.fit(train_data, train_labels, epochs=30, validation_split=0.2, verbose=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_data, test_labels, verbose=0)\n",
    "test_mae = test_results[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wykres\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.title('Mean Squared Error')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.ylim(top=1000)\n",
    "plt.ylim(bottom=0)\n",
    "print(\"Mean Absolute Error:\", test_mae)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
